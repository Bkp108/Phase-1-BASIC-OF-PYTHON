{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c77fa8fe",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§  **XGBoost** â€” eXtreme Gradient Boosting\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· What is XGBoost?\n",
    "\n",
    "- **Gradient Boosting framework** optimized for **speed and performance**.\n",
    "- Developed by **Tianqi Chen**.\n",
    "- Dominates **tabular data** problems in Kaggle and industry.\n",
    "- Designed for **scalability**, **regularization**, and **parallel processing**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Core Principles\n",
    "\n",
    "### ðŸ”¹ Boosting\n",
    "\n",
    "- An **ensemble method** that combines **weak learners** (typically decision trees).\n",
    "- Learners are trained **sequentially** to **correct errors** of previous ones.\n",
    "- Final prediction is a **weighted sum** of all learners.\n",
    "\n",
    "### ðŸ”¹ Gradient Boosting\n",
    "\n",
    "- Each tree minimizes the **gradient of the loss function**.\n",
    "- Learns **residuals** instead of absolute values.\n",
    "- Converts complex models into an **additive model** using gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Why XGBoost is Special?\n",
    "\n",
    "| Feature                   | Description                                                                 |\n",
    "|---------------------------|-----------------------------------------------------------------------------|\n",
    "| **Regularization**        | L1 (Lasso) and L2 (Ridge) help prevent overfitting                          |\n",
    "| **Sparsity-aware**        | Handles missing values efficiently                                          |\n",
    "| **Parallelization**       | Boosting is normally sequential; XGBoost parallelizes tree construction     |\n",
    "| **Cache Optimization**    | Exploits hardware to optimize computation and memory access                 |\n",
    "| **Cross-validation**      | Built-in CV support for tuning                                              |\n",
    "| **Tree Pruning**          | Uses **maximum loss reduction** for pruning instead of traditional greedy approach |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Mathematical Foundation\n",
    "\n",
    "### ðŸ”¹ Objective Function\n",
    "- Includes:\n",
    "  - **Training loss**: how well the model fits the data\n",
    "  - **Regularization term**: penalizes complexity\n",
    "- Formula:\n",
    "  ```\n",
    "  Obj = âˆ‘ loss(yáµ¢, Å·áµ¢) + âˆ‘ Î©(fâ‚–)\n",
    "  ```\n",
    "  where Î©(fâ‚–) = Î³T + Â½Î»â€–wâ€–Â²  \n",
    "  (T = number of leaves, w = leaf weights)\n",
    "\n",
    "### ðŸ”¹ Additive Model\n",
    "- Trees are added one at a time:\n",
    "  ```\n",
    "  Å·áµ¢^(t) = Å·áµ¢^(t-1) + fâ‚œ(xáµ¢)\n",
    "  ```\n",
    "\n",
    "### ðŸ”¹ Gradient + Hessian\n",
    "- Uses **2nd-order Taylor approximation** (gradient + hessian) of loss:\n",
    "  - First derivative (gáµ¢) â†’ gradient\n",
    "  - Second derivative (háµ¢) â†’ curvature\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Tree Building Strategy\n",
    "\n",
    "1. **Start with a constant prediction** (usually mean).\n",
    "2. **Compute residuals** (errors).\n",
    "3. **Build a decision tree** that predicts these residuals.\n",
    "4. **Update model** with tree predictions.\n",
    "5. Repeat steps 2â€“4.\n",
    "\n",
    "### ðŸ”¹ Split Finding\n",
    "- Uses **Gain** metric:\n",
    "  ```\n",
    "  Gain = Â½ [ (G_LÂ² / (H_L + Î»)) + (G_RÂ² / (H_R + Î»)) - (GÂ² / (H + Î»)) ] - Î³\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Regularization\n",
    "\n",
    "| Type        | Effect                                      |\n",
    "|-------------|---------------------------------------------|\n",
    "| **L1 (Î±)**  | Shrinks weights toward zero                 |\n",
    "| **L2 (Î»)**  | Smooths weights, avoids overfitting         |\n",
    "| **Î³ (gamma)** | Minimum loss reduction to make a split     |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Handling Missing Values\n",
    "\n",
    "- Learns the **optimal default direction** for missing values during tree training.\n",
    "- No need for imputation before training.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Hyperparameters (Theory)\n",
    "\n",
    "| Hyperparameter        | Role                                                    |\n",
    "|------------------------|---------------------------------------------------------|\n",
    "| `n_estimators`         | Number of trees                                         |\n",
    "| `max_depth`            | Maximum depth of each tree                              |\n",
    "| `learning_rate` (Î·)    | Shrinks the contribution of each tree                   |\n",
    "| `subsample`            | Fraction of data used per tree                          |\n",
    "| `colsample_bytree`     | Fraction of features used per tree                      |\n",
    "| `gamma`                | Minimum loss reduction required for split               |\n",
    "| `lambda`, `alpha`      | L2 and L1 regularization                                |\n",
    "| `min_child_weight`     | Minimum sum of instance weight (hessian) in a child     |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Evaluation Metrics\n",
    "\n",
    "- For classification: `logloss`, `error`, `auc`, `f1`\n",
    "- For regression: `rmse`, `mae`, `rmsle`\n",
    "- Can use **custom loss** and metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Use Cases\n",
    "\n",
    "| Domain              | Use Case                              |\n",
    "|---------------------|----------------------------------------|\n",
    "| Finance             | Credit scoring, fraud detection        |\n",
    "| Healthcare          | Disease prediction                     |\n",
    "| Marketing           | Churn prediction                       |\n",
    "| Competition         | Tabular data (Kaggle, analytics)       |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Strengths\n",
    "\n",
    "- Fast and scalable\n",
    "- Regularization = better generalization\n",
    "- Handles missing data\n",
    "- Works well with default settings\n",
    "- Extremely flexible and customizable\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Weaknesses\n",
    "\n",
    "- Not interpretable (trees are complex)\n",
    "- Can overfit if not tuned\n",
    "- Requires tabular, numeric input (manual preprocessing)\n",
    "- Not optimal for image/audio/text (use neural nets there)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Summary\n",
    "\n",
    "- **XGBoost = Gradient Boosting + Engineering Excellence**\n",
    "- Think of it as the **Ferrari of decision tree ensembles**.\n",
    "- When itâ€™s **tabular**, go **XGBoost**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb0f965",
   "metadata": {},
   "source": [
    "âš¡ **XGBOOST MASTERDUMP** â€” Pure, blazing insight on eXtreme Gradient Boosting. Fast, powerful, scalable.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 1. Install & Import\n",
    "\n",
    "```bash\n",
    "pip install xgboost\n",
    "```\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 2. Why XGBoost?\n",
    "\n",
    "- Ensemble of decision trees (boosted trees).\n",
    "- Handles **missing values**, **categorical data**, and **imbalanced datasets**.\n",
    "- Fast due to parallel processing.\n",
    "- Regularization to reduce overfitting (`alpha`, `lambda`).\n",
    "- Works for both **classification** and **regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 3. Basic Model (Classification)\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 4. Regression\n",
    "\n",
    "```python\n",
    "model = XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 5. DMatrix API (Advanced, Faster)\n",
    "\n",
    "```python\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic'\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "preds = model.predict(dtest)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 6. Common Parameters\n",
    "\n",
    "| Param            | Meaning                                  |\n",
    "|------------------|------------------------------------------|\n",
    "| `n_estimators`   | Number of trees (boosting rounds)        |\n",
    "| `max_depth`      | Max depth of each tree                   |\n",
    "| `learning_rate` / `eta` | Shrinkage rate to control overfit   |\n",
    "| `subsample`      | % of rows sampled per tree               |\n",
    "| `colsample_bytree` | % of cols sampled per tree             |\n",
    "| `gamma`          | Min loss reduction to split a node       |\n",
    "| `lambda`         | L2 regularization                        |\n",
    "| `alpha`          | L1 regularization                        |\n",
    "| `scale_pos_weight` | For imbalanced classes                |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 7. Evaluation Metrics\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "mean_squared_error(y_test, y_pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 8. Early Stopping\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(n_estimators=500)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 9. Feature Importance\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 10. Save / Load Model\n",
    "\n",
    "```python\n",
    "model.save_model(\"model.json\")\n",
    "model.load_model(\"model.json\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 11. Handle Missing Values\n",
    "\n",
    "```python\n",
    "# XGBoost handles np.nan internally\n",
    "X_train[X_train > 1000] = np.nan\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 12. Categorical Features\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X_cat = encoder.fit_transform(X_categorical)\n",
    "```\n",
    "\n",
    "(Or use `XGBClassifier(tree_method='hist', enable_categorical=True)` with proper dtype from v1.5+)\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 13. Use with Sklearn Pipeline + GridSearch\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(XGBClassifier(), param_grid=params, scoring='accuracy', cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 14. SHAP for Interpretability\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 15. Use GPU (if available)\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 16. Multi-Class Classification\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 17. Logloss & AUC\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'auc']\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 18. Custom Objective / Metric\n",
    "\n",
    "```python\n",
    "def custom_logloss(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    grad = y_pred - y_true\n",
    "    hess = y_pred * (1 - y_pred)\n",
    "    return grad, hess\n",
    "\n",
    "model = xgb.train(params, dtrain, obj=custom_logloss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 19. Cross Validation\n",
    "\n",
    "```python\n",
    "xgb.cv(params, dtrain, nfold=5, metrics={'logloss'}, early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… 20. XGB vs Others\n",
    "\n",
    "| Model      | Pros                                  | Cons                    |\n",
    "|------------|----------------------------------------|-------------------------|\n",
    "| XGBoost    | Fast, accurate, robust to outliers     | Can overfit             |\n",
    "| LightGBM   | Faster on large datasets               | Categorical handling    |\n",
    "| CatBoost   | Native categorical support             | Slightly slower         |\n",
    "| RandomForest | Simple, interpretable trees         | Less accurate sometimes |\n",
    "\n",
    "---\n",
    "\n",
    "XGBoost = **the killer ensemble**. Train fast. Tune hard. Dominate Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf35f7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
