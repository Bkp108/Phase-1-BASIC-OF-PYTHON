{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab0e029e",
   "metadata": {},
   "source": [
    "\n",
    "# üß† **PyTorch** ‚Äì The Deep Learning Framework of Choice\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ What is PyTorch?\n",
    "\n",
    "- An **open-source deep learning framework** developed by **Facebook's AI Research lab (FAIR)**.\n",
    "- Focused on **flexibility**, **dynamic computation**, and **Pythonic interface**.\n",
    "- Competes with TensorFlow, but more popular in research due to ease of debugging and readability.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Core Concepts\n",
    "\n",
    "### üîπ 1. **Tensors**\n",
    "\n",
    "- Fundamental data structure, similar to NumPy arrays but with GPU support.\n",
    "- Can be scalars, vectors, matrices, or higher-dimensional (N-D) arrays.\n",
    "- Supports automatic differentiation (`requires_grad=True`).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. **Dynamic Computational Graphs**\n",
    "\n",
    "- Also known as **Define-by-Run**.\n",
    "- The computation graph is created **on-the-fly** at each forward pass.\n",
    "- Allows easy debugging and flexible model structures.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. **Autograd**\n",
    "\n",
    "- PyTorch‚Äôs **automatic differentiation engine**.\n",
    "- Tracks all operations on tensors with `requires_grad=True`.\n",
    "- Builds a **computational graph**, stores gradients, and computes derivatives during backpropagation.\n",
    "- Backprop is triggered using `.backward()`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 4. **nn.Module**\n",
    "\n",
    "- Base class for **neural network models**.\n",
    "- Custom models subclass `nn.Module` and define layers in `__init__()` and computation logic in `forward()`.\n",
    "- Automatically tracks parameters and moves them across devices (CPU/GPU).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 5. **Optimizers**\n",
    "\n",
    "- Part of `torch.optim`.\n",
    "- Updates model parameters using gradients from `autograd`.\n",
    "- Common optimizers: `SGD`, `Adam`, `RMSprop`.\n",
    "- Requires `model.parameters()` and a `learning_rate`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 6. **Loss Functions**\n",
    "\n",
    "- Defined in `torch.nn` module.\n",
    "- Measures the error between prediction and actual.\n",
    "- Examples:\n",
    "  - `CrossEntropyLoss` ‚Üí classification\n",
    "  - `MSELoss` ‚Üí regression\n",
    "  - Custom loss functions also possible.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 7. **Data Loading (torch.utils.data)**\n",
    "\n",
    "- Data handled via:\n",
    "  - `Dataset` ‚Üí defines how to access items\n",
    "  - `DataLoader` ‚Üí handles batching, shuffling, multiprocessing\n",
    "- Enables scalable training with huge datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 8. **GPU Acceleration (CUDA)**\n",
    "\n",
    "- Use `.cuda()` or `.to(device)` to move tensors/models to GPU.\n",
    "- PyTorch leverages NVIDIA‚Äôs CUDA for parallel computation.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Model Training Workflow (Theoretical Steps)\n",
    "\n",
    "1. **Define model** using `nn.Module`.\n",
    "2. **Prepare data** using `Dataset` and `DataLoader`.\n",
    "3. **Choose loss function** (criterion).\n",
    "4. **Choose optimizer** and connect to model params.\n",
    "5. Loop:\n",
    "   - Forward pass\n",
    "   - Compute loss\n",
    "   - Backward pass (`loss.backward()`)\n",
    "   - Optimizer step (`optimizer.step()`)\n",
    "   - Zero gradients (`optimizer.zero_grad()`)\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Ecosystem Components\n",
    "\n",
    "| Component        | Purpose                                       |\n",
    "|------------------|-----------------------------------------------|\n",
    "| **TorchVision**  | Pre-trained models, image datasets, transforms |\n",
    "| **TorchText**    | NLP utilities                                 |\n",
    "| **TorchAudio**   | Audio processing                              |\n",
    "| **ONNX**         | Model export for cross-framework compatibility|\n",
    "| **TorchScript**  | Serializes models for deployment              |\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Key Advantages\n",
    "\n",
    "- **Pythonic & Intuitive** ‚Äî seamless with Python code\n",
    "- **Dynamic Graphs** ‚Äî ideal for RNNs, variable input\n",
    "- **Strong GPU Support** ‚Äî out-of-the-box CUDA integration\n",
    "- **Modular Design** ‚Äî build custom layers/ops easily\n",
    "- **Huge Community & Adoption** ‚Äî used in academia and industry\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Limitations\n",
    "\n",
    "- Can be verbose for low-level control\n",
    "- Deployment and mobile support less mature than TensorFlow (though catching up fast)\n",
    "- Higher memory usage if not optimized\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Use Cases\n",
    "\n",
    "- **Image classification** (ResNet, VGG, etc.)\n",
    "- **Object detection & segmentation**\n",
    "- **Natural Language Processing** (Transformer models)\n",
    "- **Reinforcement learning**\n",
    "- **GANs & generative models**\n",
    "- **Time-series forecasting**\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ PyTorch vs TensorFlow (Theoretical Comparison)\n",
    "\n",
    "| Feature               | PyTorch                     | TensorFlow                  |\n",
    "|------------------------|-----------------------------|-----------------------------|\n",
    "| Computation Graph      | Dynamic                     | Static + Eager              |\n",
    "| Debugging              | Intuitive                   | Complex                     |\n",
    "| Syntax                 | Pythonic                    | More verbose                |\n",
    "| Popularity (Research)  | Very high                   | Improving                   |\n",
    "| Deployment (Prod)      | Medium (TorchScript, ONNX)  | Excellent (TF Serving)      |\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ Summary\n",
    "\n",
    "- PyTorch is **flexible**, **clean**, and **powerful**.\n",
    "- Ideal for **research**, **experimentation**, and **prototyping**.\n",
    "- The choice when you want to ‚Äúbuild deep learning with full control‚Äù.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b72172",
   "metadata": {},
   "source": [
    "‚ö° **PYTORCH MASTERDUMP** ‚Äî Core torch knowledge, piped directly into your neural net. \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Install & Import\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets, models\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. Tensors\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "x = torch.zeros((2, 3))\n",
    "x = torch.ones((3, 3))\n",
    "x = torch.randn((4, 4))\n",
    "x = torch.eye(3)  # Identity matrix\n",
    "```\n",
    "\n",
    "### GPU\n",
    "\n",
    "```python\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. Autograd\n",
    "\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2 + 3*x + 1\n",
    "y.backward()\n",
    "x.grad  # Gradient of y w.r.t x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. Build a Model (Sequential)\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. Custom Model (OOP)\n",
    "\n",
    "```python\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = MyModel()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 6. Loss & Optimizer\n",
    "\n",
    "```python\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 7. Training Loop\n",
    "\n",
    "```python\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 8. Evaluation\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test.to(device))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 9. Dataset & DataLoader\n",
    "\n",
    "```python\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(CustomDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 10. torchvision Datasets (Images)\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder('path/to/data', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 11. CNN\n",
    "\n",
    "```python\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(16 * 62 * 62, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 12. Transfer Learning\n",
    "\n",
    "```python\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # New output layer\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 13. Save / Load Model\n",
    "\n",
    "```python\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 14. Common Losses\n",
    "\n",
    "```python\n",
    "nn.CrossEntropyLoss()       # multi-class\n",
    "nn.BCELoss()                # binary\n",
    "nn.MSELoss()                # regression\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 15. Common Optimizers\n",
    "\n",
    "```python\n",
    "torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 16. GPU Training Pattern\n",
    "\n",
    "```python\n",
    "model.to(device)\n",
    "X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 17. One-Hot Encoding\n",
    "\n",
    "```python\n",
    "y_onehot = F.one_hot(y, num_classes=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 18. Metrics (Manual)\n",
    "\n",
    "```python\n",
    "accuracy = (pred.argmax(dim=1) == y).float().mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 19. Mixed Precision Training\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "for data, target in loader:\n",
    "    with autocast():\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 20. Lightning / HuggingFace (level-up)\n",
    "\n",
    "- Use **PyTorch Lightning** for cleaner training.\n",
    "- Use **transformers** from HuggingFace for NLP & pre-trained DL models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d302aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
